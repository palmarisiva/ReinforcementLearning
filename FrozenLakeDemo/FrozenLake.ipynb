{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that \n",
    "left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted.\n",
    "If you step into one of those holes, you'll fall into the freezing water. \n",
    "At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake \n",
    "and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend. \n",
    "The surface is described using a grid like the following:\n",
    "\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This grid is our environment where S is the agent’s starting point, and it’s safe. F represents the frozen surface and is also safe. H represents a hole, and if our agent steps in a hole in the middle of a frozen lake, well, that’s not good. Finally, G represents the goal, which is the space on the grid where the prized frisbee is located.\n",
    "\n",
    "The agent can navigate left, right, up, and down, and the episode ends when the agent reaches the goal or falls in a hole. It receives a reward of one if it reaches the goal, and zero otherwise.\n",
    "\n",
    "State\tDescription\tReward\n",
    "S\tAgent’s starting point - safe\t0\n",
    "F\tFrozen surface - safe\t0\n",
    "H\tHole - game over\t0\n",
    "G\tGoal - game over\t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "#Initialize the q_table\n",
    "q_table = np.zeros((state_space_size,action_space_size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Q-Learning Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "max_steps_per_episode =100\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "exploration_rate = 1\n",
    "max_exploration_rate =  1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_all_episodes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all the episodes \n",
    "for episode in range(num_episodes):\n",
    "    # Resetting the environment for each episode and the episode has not ended by using done variable and rewards_current_episode as 0\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    # Loop through each step per episode.\n",
    "    for step in range(max_steps_per_episode): \n",
    "        #For each time-step within an episode, we set our exploration_rate_threshold to a random number between 0 and 1. This will be used to determine whether our agent will explore or exploit the environment in this time-step\n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        #If the threshold is greater than the exploration_rate, which remember, is initially set to 1, then our agent will exploit the environment and choose the action that has the highest Q-value in the Q-table for the current state.  \n",
    "        if(exploration_rate_threshold > exploration_rate):\n",
    "            action = np.argmax(q_table[state,:])\n",
    "        else:\n",
    "            #the agent will explore the environment, and sample an action randomly.\n",
    "            action =  env.action_space.sample()\n",
    "            #Take that action by calling step() on our env object and passing our action to it\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        #update the Q value\n",
    "        q_table[state, action] = q_table[state,action] * (1-learning_rate) + learning_rate*(reward + discount_rate*np.max(q_table[new_state,:]))\n",
    "        state = new_state\n",
    "        rewards_current_episode += reward\n",
    "        if done == True: \n",
    "            break\n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    rewards_all_episodes.append(rewards_current_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.01900000000000001\n",
      "2000 :  0.01900000000000001\n",
      "3000 :  0.028000000000000018\n",
      "4000 :  0.03300000000000002\n",
      "5000 :  0.03800000000000003\n",
      "6000 :  0.04300000000000003\n",
      "7000 :  0.060000000000000046\n",
      "8000 :  0.08900000000000007\n",
      "9000 :  0.10200000000000008\n",
      "10000 :  0.10200000000000008\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.47857402 0.45673019 0.44683036 0.45397963]\n",
      " [0.28675429 0.24693639 0.22332269 0.36779719]\n",
      " [0.31511233 0.27572929 0.28724263 0.27804774]\n",
      " [0.09591841 0.1920452  0.06280701 0.10178862]\n",
      " [0.48755724 0.39613361 0.22822045 0.41665569]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.11386927 0.18522519 0.29295839 0.08119003]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.41541809 0.31879924 0.27609227 0.5120276 ]\n",
      " [0.38343411 0.54712532 0.36512931 0.31852695]\n",
      " [0.54317864 0.36385362 0.33689954 0.37188045]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.37765347 0.49803681 0.69345935 0.43302213]\n",
      " [0.66871024 0.85350789 0.74855357 0.71962128]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
